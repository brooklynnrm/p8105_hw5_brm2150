---
title: "p8105_hw5_brm2150"
author: "Brooklynn McNeil"
date: "2024-11-03"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  comment = '', fig.width = 8, fig.height = 6, out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Let's put people in a room. Going to assume that no one was born on leap day and birthdays are normally distributed across the year.

Create a function for size (n) that will tell us if there is a duplicate birthday in the group.
```{r}
bday_sim = function(n) {
  
  bdays = sample(1:365, size = n, replace = TRUE)

  duplicate = length(unique(bdays)) < n
  
  return(duplicate)
  
}

bday_sim(50)
```

Now let's iterate this for 2 to 50 people in the room. Then iterate 10,000 times, saving each in a data frame. 
```{r}
sim_res = 
  expand_grid(
    n = c(2:50),
    iter = 1:10000
  ) |>
  mutate(res = map_lgl(n, bday_sim)) |>
  group_by(n) |>
  summarize(prob = mean(res))
```

Now let's plot the results as a function of n. We see that as n increases the probability of having two people in the same room having the same birthday also increases. after about only 22 people are in the room our chance of success is about 50%.
```{r}
sim_res |>
  ggplot(aes(x = n, y = prob)) +
  geom_line() +
  labs(title = "Probability of Having the Same Bday")
```


## Problem 2

Let's create a function for creating a normal distribution and then do a t.test.
```{r}
rnorm_t.test = function(u){
  
  samp = rnorm(n = 30, mean = u, sd = 5)
  
  broom::tidy(t.test(samp, mu =0, conf.level = 0.95))
}

rnorm_t.test(u=0)
```

Now let's create a simulation for different mu values and 5000 iterations each.
```{r}
sim_samp = 
  expand_grid(
    mean = 0:6,
    iter = 1:5000) |>
  mutate(samp = map(mean,rnorm_t.test)) |>
  unnest(samp)

```

Plot the true mean versus the power of the test, the number of times the null was rejected. 

```{r}
power_data = 
  sim_samp |>
  group_by(mean) |>
  summarize(power = mean(p.value< 0.05))

power_data |>
  ggplot(aes(x = mean, y = power)) +
  geom_point() +
  geom_line() +
  labs(title = "Power vs. True Mean")
```

Plot the estimate versus the true mean. and then the estimate only for times when the null was rejected. 

```{r}
estimates = 
  sim_samp |>
  group_by(mean) |>
  summarize(avg_mean = mean(estimate),
            avg_mean_rejected = mean(estimate[p.value < 0.05]))

estimates |>
  ggplot(aes(x = mean)) +
  geom_line(aes(y = avg_mean, color = "All Samples")) +
  geom_line(aes(y = avg_mean_rejected, color = "Null Rejected")) +
  labs(title = "True Mean vs. Average Mean",
       x = "True Mean",
       y = "Average Mean",
       color = "Sample Type")
```
The average mean for all samples is very close to the true mean, but is quite skewed for the subset that rejected the null. Rejecting the null caused a skew in the data, especially for true mean of 1 and 2. As the true mean increases, the selection bias diminishes.

## Problem 3

```{r}
url = "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
homicide_dat = 
  read_csv(url, na = "Unknown") |>
  mutate(city_state = str_c(city, "_", state))

head(homicide_dat)
```
The raw data contains information about homicides across 50 cities. The range of victim age is `r range(homicide_dat$victim_age)`. Below is a table of total homicides for each city as well as the unsolved murders.

```{r}
homicide_dat |>
  group_by(city_state) |>
  summarize(Total_Homicides = n(),
            Unsolved_Homcides = sum(disposition %in% c("Open/No arrest", "Closed without arrest")))|>
  knitr::kable()
```

Let's look at the proportion of murders solved in Baltimore, Maryland.
```{r}
baltimore_dat = 
  homicide_dat |>
  filter(city_state == "Baltimore_MD") |>
  mutate(status = case_when(
    disposition %in% c("Closed without arrest", "Closed by arrest") ~ 0,
    disposition == "Open/No arrest" ~ 1
  )) 
total = 
  baltimore_dat |>
  drop_na(status)|>
  nrow()
unsolved = 
  baltimore_dat |> 
  pull(status) |> 
  sum()

prop.test(unsolved, total) |>
  broom::tidy() |>
  select(estimate, conf.low, conf.high)
```

Now let's map a prop.test across each city.

```{r}
city_summary = 
  homicide_dat |>
  mutate(status = case_when(
    disposition %in% c("Closed without arrest", "Closed by arrest") ~ 0,
    disposition == "Open/No arrest" ~ 1),
    city_state = as.factor(str_c(city, "_", state))
    )|>
  group_by(city_state) |>
  summarize(total = n(),
            unsolved = sum(status))

city_prop = 
  city_summary |>
  mutate(prop_test = map2(.x = unsolved, .y = total, ~ prop.test(.x, .y) |> broom::tidy())
         ) |>
  unnest(prop_test)|>
  select(city_state, estimate, conf.low, conf.high)

city_prop |> 
  knitr::kable(digits = 3)
```

Plotting the cities with the proportion of unsolved homicides with confidence intervals.

```{r}
city_prop |>
  arrange(desc(estimate)) |>
  mutate(city_state = factor(city_state, levels = city_state)) |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1))+
  labs(
    title = "Proportion of Unsolved Homicides by City",
    x = "City and State",
    y = "Estimated Proportion of unsolved Homicides"
  )
```

